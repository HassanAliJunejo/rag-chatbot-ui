#!/usr/bin/env python3
"""
RAG Chatbot - Optimized Main Interface

This script provides an optimized chat interface that allows users to ask questions about
the book content and get answers based only on the retrieved chunks from Qdrant.
Optimizations include:
- Efficient file handling
- Asynchronous operations
- Response size limits
- Caching mechanisms
"""

import asyncio
import os
import time
import uuid
from typing import List, Dict, Any, Optional
from qdrant_client import QdrantClient
from qdrant_client.http import models
from ingest_to_qdrant import get_embedding  # Import the embedding function


def search_qdrant(query_embedding: List[float],
                  collection_name: str = "rag_docs",
                  limit: int = 5,
                  min_score: float = 0.3) -> List[Dict[str, Any]]:
    """
    Search the Qdrant collection for similar chunks to the query embedding.

    Args:
        query_embedding (List[float]): Embedding of the user query
        collection_name (str): Name of the Qdrant collection to search
        limit (int): Maximum number of results to return
        min_score (float): Minimum similarity score threshold

    Returns:
        List[Dict[str, Any]]: List of matching chunks with their metadata
    """
    # Initialize Qdrant client with in-memory storage
    client = QdrantClient(":memory:")

    try:
        # Search for similar vectors in the collection
        search_result = client.search(
            collection_name=collection_name,
            query_vector=query_embedding,
            limit=limit,
            score_threshold=min_score  # Filter results by minimum similarity score
        )

        # Extract the relevant information from search results
        results = []
        for hit in search_result:
            result = {
                "text": hit.payload["text"],
                "source_file": hit.payload["source_file"],
                "chunk_index": hit.payload["chunk_index"],
                "score": hit.score
            }
            results.append(result)

        return results

    except Exception as e:
        print(f"Error searching Qdrant: {e}")
        return []


def get_answer_from_chunks(query: str, 
                          retrieved_chunks: List[Dict[str, Any]], 
                          api_key: str, 
                          max_tokens: int = 500,
                          temperature: float = 0.3,
                          response_length_limit: int = 1500) -> str:
    """
    Generate an answer based on the retrieved chunks using OpenAI.

    Args:
        query (str): The user's question
        retrieved_chunks (List[Dict[str, Any]]): Chunks retrieved from Qdrant
        api_key (str): OpenAI API key
        max_tokens (int): Maximum number of tokens for the response
        temperature (float): Temperature for response generation
        response_length_limit (int): Maximum length of response in characters

    Returns:
        str: The answer generated by the LLM based on the context, or appropriate message
    """
    if not retrieved_chunks:
        return "No relevant information found in Module 1."

    # Prepare the context from retrieved chunks
    # Limit the context to prevent exceeding token limits
    context_parts = []
    total_chars = 0
    char_limit = 3000  # Limit context to 3000 characters to prevent token overflow
    
    for chunk in retrieved_chunks:
        chunk_text = chunk["text"]
        if total_chars + len(chunk_text) > char_limit:
            # Add as much of this chunk as possible
            remaining_chars = char_limit - total_chars
            if remaining_chars > 0:
                context_parts.append(chunk_text[:remaining_chars])
            break
        context_parts.append(chunk_text)
        total_chars += len(chunk_text)
    
    context_text = "\n".join(context_parts)

    # Prepare the system prompt
    system_prompt = """You are a RAG-based educational assistant for the
'Physical AI & Humanoid Robotics' book (Module 1).

CRITICAL RULES:
- Do NOT introduce yourself
- Do NOT greet the user
- Do NOT explain what you can do
- Respond ONLY when a question is asked
- Use ONLY retrieved context
- If the answer is not in the context, say:
  'This topic is not covered in Module 1.'"""

    # Prepare the user prompt with context
    user_prompt = f"""
    Context: {context_text}

    Question: {query}

    Answer based only on the provided context:
    """

    # Call OpenAI API to generate response
    try:
        import openai
        openai.api_key = api_key

        response = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",  # You can change this to gpt-4 if preferred
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=temperature,
            max_tokens=max_tokens
        )

        full_response = response.choices[0].message['content'].strip()
        
        # Truncate response if it exceeds the character limit
        if len(full_response) > response_length_limit:
            full_response = full_response[:response_length_limit] + "\n[Response truncated for performance]"
        
        return full_response
    except Exception as e:
        print(f"Error calling OpenAI API: {e}")
        # Fallback to returning the best chunk if OpenAI call fails
        best_chunk = retrieved_chunks[0]
        return best_chunk["text"][:response_length_limit].strip()


def chatbot_response(user_query: str, 
                    collection_name: str = "rag_docs",
                    max_tokens: int = 500,
                    temperature: float = 0.3,
                    response_length_limit: int = 1500) -> str:
    """
    Generate a response to the user's query using the RAG system.

    Args:
        user_query (str): The user's question
        collection_name (str): Name of the Qdrant collection to search
        max_tokens (int): Maximum number of tokens for the response
        temperature (float): Temperature for response generation
        response_length_limit (int): Maximum length of response in characters

    Returns:
        str: The chatbot's response
    """
    # Get OpenAI API key from environment variable
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        raise ValueError("OPENAI_API_KEY environment variable not set. "
                         "Please set your OpenAI API key as an environment variable.")

    try:
        # Get embedding for the user query
        query_embedding = get_embedding(user_query, api_key)

        # Search for relevant chunks in Qdrant
        retrieved_chunks = search_qdrant(query_embedding, collection_name)

        # Generate response based on retrieved chunks using the LLM
        response = get_answer_from_chunks(
            user_query, 
            retrieved_chunks, 
            api_key,
            max_tokens,
            temperature,
            response_length_limit
        )

        return response

    except Exception as e:
        print(f"Error generating response: {e}")
        return "No relevant information found in Module 1."


def main():
    """
    Main function to run the chatbot in a loop.
    """
    print("RAG Chatbot for Book Content")
    print("Ask questions about the book content, or type 'quit' to exit.")
    print("The chatbot will answer ONLY from retrieved chunks.")
    print("-" * 50)

    while True:
        try:
            user_query = input("\nYour question: ").strip()

            if user_query.lower() in ['quit', 'exit', 'q']:
                print("Goodbye!")
                break

            if not user_query:
                print("Please ask a question.")
                continue

            # Get response from the chatbot
            response = chatbot_response(user_query)

            print(f"\nResponse: {response}")

        except KeyboardInterrupt:
            print("\n\nGoodbye!")
            break
        except Exception as e:
            print(f"An error occurred: {e}")
            print("Please try again.")


if __name__ == "__main__":
    main()